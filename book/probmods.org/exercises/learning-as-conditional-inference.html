<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from probmods.org/exercises/learning-as-conditional-inference.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 04 Feb 2020 14:10:23 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>learning - exercises</title>

    <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="../assets/css/bootstrap-theme.min.css">
    <link rel="stylesheet" href="../assets/css/default.css">
    <link href="https://fonts.googleapis.com/css?family=Crimson+Text|Inconsolata" rel="stylesheet">
    <script src="../assets/js/ga.js"></script>
    <script src="../assets/js/jquery.min.js"></script>
    <script type="text/javascript" src="../assets/js/bootstrap.min.js"></script>

<!--    <script src="/assets/js/underscore-min.js"></script> 
    <script src="https://probmods.org/bower_components/underscore/underscore.js"></script> -->

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    
      
      <link rel="stylesheet" href="../assets/css/katex.min.css" media="screen" type="text/css">
      
      <link rel="stylesheet" href="../assets/css/littlefoot.css" media="screen" type="text/css">
      
      <link rel="stylesheet" href="../assets/css/webppl-viz.css" media="screen" type="text/css">
      
      <link rel="stylesheet" href="../assets/css/webppl-editor.css" media="screen" type="text/css">
      
    
    
    
    
      
      <script src='../assets/js/katex.min.js' type="text/javascript"></script>
      
      <script src='../assets/js/littlefoot.min.js' type="text/javascript"></script>
      
      <script src='../assets/js/paper-full.js' type="text/javascript"></script>
      
      <script src='../assets/js/parse-bibtex.js' type="text/javascript"></script>
      
      <script src='../assets/js/chapter.js' type="text/javascript"></script>
      
    
    
    
    
  </head>
  <body>

    

    <div class="container">
      <ul class="nav navbar-nav">
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">&#9776;</a>
          <ul class="dropdown-menu">
            <li><a href="../index.html">Home</a></li>
            <li role="separator" class="divider"></li>
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            <li><a href="../chapters/introduction.html">Introduction</a></li>
            
            
            
            <li><a href="../chapters/generative-models.html">Generative models</a></li>
            
            
            
            <li><a href="../chapters/conditioning.html">Conditioning</a></li>
            
            
            
            <li><a href="../chapters/dependence.html">Causal and statistical dependence</a></li>
            
            
            
            <li><a href="../chapters/conditional-dependence.html">Conditional dependence</a></li>
            
            
            
            <li><a href="../chapters/bayesian-data-analysis.html">Bayesian data analysis</a></li>
            
            
            
            <li><a href="../chapters/inference-algorithms.html">Algorithms for inference</a></li>
            
            
            
            <li><a href="../chapters/process-models.html">Rational process models</a></li>
            
            
            
            <li><a href="../chapters/learning-as-conditional-inference.html">Learning as conditional inference</a></li>
            
            
            
            <li><a href="../chapters/lot-learning.html">Learning with a language of thought</a></li>
            
            
            
            <li><a href="../chapters/hierarchical-models.html">Hierarchical models</a></li>
            
            
            
            <li><a href="../chapters/occams-razor.html">Occam's Razor</a></li>
            
            
            
            <li><a href="../chapters/function-learning.html">Learning (deep) continuous functions</a></li>
            
            
            
            <li><a href="../chapters/mixture-models.html">Mixture models</a></li>
            
            
            
            <li><a href="../chapters/social-cognition.html">Social cognition</a></li>
            
            
            
            <li><a href="../chapters/appendix-js-basics.html">Appendix - JavaScript basics</a></li>
            
            
            
            <li><a href="../chapters/appendix-useful-distributions.html">Appendix - Useful distributions</a></li>
            
            
          </ul>
        </li>
      </ul>

      <div class="page-header">
  <h1>learning - exercises</h1>
</div>

<h2 id="1-our-prior-beliefs-about-coins">1. Our prior beliefs about coins</h2>

<h4 id="a">a)</h4>

<p>Recall our final coin weight model, “fair-vs-uniform”, in which the coin weight was either 0.5 with high probability or drawn from a uniform distribution otherwise. This implies that a two-faced coin (always heads) is equally likely as a 70% heads coin. Intuitively you might be inclined to think that a two-faced coin is easier to make, and thus more likely. Adjust the model to express this prior.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var weightPosterior = function(observedData){
  return Infer({method: 'MCMC', burn:1000, samples: 10000}, function() {
    //
    var realWeight = //..your code here
    var coin = Bernoulli({p: realWeight})
    var obsFn = function(datum){observe(coin, datum=='h')}
    mapData({data: observedData}, obsFn)
    return realWeight
  })
}

var fullDataSet = repeat(50, function(){return 'h'});
var observedDataSizes = [0,1,2,4,6,8,10,12,15,20,25,30,40,50];
var estimates = map(function(N) {
  return expectation(weightPosterior(fullDataSet.slice(0,N)))
}, observedDataSizes);
viz.line(observedDataSizes, estimates);
</code></pre></div></div>

<h4 id="b">b)</h4>

<p>How does your solution behave differently than the fair-vs-uniform model from the chapter? Find a data set such that the learning curves are qualitatively different.</p>

<h2 id="2-the-strength-of-beliefs">2. The strength of beliefs</h2>

<!--
  NDG: i removed this for now, because it's not explained in chapter. is it a real distiction?
How does a *learning curve* differ from a *learning trajectory*?
-->

<h4 id="a-1">a)</h4>

<p>In the chapter, we graphed <em>learning trajectories</em> for a number of models. Below is one of these models (the one with the <code class="highlighter-rouge">Beta(10,10)</code> prior). In the chapter, we observed how the model’s best guess about the weight of the coin changed across a sequence of sucessive heads. See what happens if instead we see heads and tails in alternation:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var pseudoCounts = {a: 10, b: 10};

var weightPosterior = function(observedData){
  return Infer({method: 'MCMC', burn:1000, samples: 1000}, function() {
    var coinWeight = beta(pseudoCounts))
    var coin = Bernoulli({p: coinWeight})
    var obsFn = function(datum){observe(coin, datum=='h')}
    mapData({data: observedData}, obsFn)
    return coinWeight
  })
}

//creating 50 pairs of 'h' and 't' alternating
var fullDataSet = repeat(50,function(){['h', 't']}).flat()

var observedDataSizes = [0,2,4,6,8,10,20,30,40,50,70,100]
var estimates = map(function(N) {
  return expectation(weightPosterior(fullDataSet.slice(0,N)))
}, observedDataSizes)
viz.line(observedDataSizes, estimates);
</code></pre></div></div>

<p>It looks like we haven’t learned anything! Indeed, since our best estimate for the coin’s weight was 0.5 <em>prior</em> to observing anything, our best estimate is hardly going to change when we get data consistent with that prior.</p>

<p>However, we’ve been looking at the average (or expected) estimate. Edit the code below to see whether our posterior <em>distribution</em> is at all changed by observing this data set. (You only need to compare the prior and the posterior after all 100 observations):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var pseudoCounts = {a: 10, b: 10};

var weightPosterior = function(observedData){
  return Infer({method: 'MCMC', burn:1000, samples: 1000}, function() {
    var coinWeight = beta(pseudoCounts))
    var coin = Bernoulli({p: coinWeight})
    var obsFn = function(datum){observe(coin, datum=='h')}
    mapData({data: observedData}, obsFn)
    return coinWeight
  })
}

//creating 50 pairs of 'h' and 't' alternating
var fullDataSet = repeat(50,function(){['h', 't']}).flat()

var prior = //your code here
var post = //your code here

viz(prior) //should graph the prior distribution on weights
viz(post) //should graph the posterior distribution on weights
</code></pre></div></div>

<p>Based on your results, is anything learned? (Note that the bounds on the x-axis are likely to be different in the two graphs, which could obscure this. The <code class="highlighter-rouge">viz</code> package doesn’t easily to allow you to adjust the bounds on the axes.)</p>

<h4 id="b-1">b)</h4>

<p>Ideally, we’d like to see how our belief distribution shifts as more data comes in. A particularly good measure would be entropy. Unfortunately, calculating entropy for a Beta distribution is <a href="https://en.wikipedia.org/wiki/Beta_distribution#Quantities_of_information_(entropy)">somewhat involved</a>.</p>

<p>An alternative we can use is variance: the expected squared difference between a sample from the distribution and the distribution mean. This doesn’t take into account the shape of the distribution, and so won’t give us quite what we want if the distribution is non-symmetric; but it is a reasonable first try.</p>

<p>Edit the code below to see how variance changes as more data is observed.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var pseudoCounts = {a: 10, b: 10};

var weightPosterior = function(observedData){
  return Infer({method: 'MCMC', burn:1000, samples: 1000}, function() {
    var coinWeight = beta(pseudoCounts))
    var coin = Bernoulli({p: coinWeight})
    var obsFn = function(datum){observe(coin, datum=='h')}
    mapData({data: observedData}, obsFn)
    return coinWeight
  })
}

//creating 50 pairs of 'h' and 't' alternating
var fullDataSet = repeat(50,function(){['h', 't']}).flat()


var observedDataSizes = [0,2,4,8,16,32,64,128,256,512];
var posts = map(function(N) {
  return weightPosterior(fullDataSet.slice(0,N))
}, observedDataSizes); 
// returns an array of posteriors of length observedDataSizes.length

var variances = map(function(p){
  // your code here
}, posts)

viz.line(observedDataSizes, variances);
</code></pre></div></div>

<p>HINT: notice how the variable <code class="highlighter-rouge">posts</code> differs from <code class="highlighter-rouge">estimates</code> in the code above.</p>

<h2 id="3-causal-power">3. Causal Power</h2>

<p>Consider our model of causal power from the chapter:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var observedData = [{C:true, E:false}]

var causalPowerPost = Infer({method: 'MCMC', samples: 10000, lag:2}, function() {
  // Causal power of C to cause E
  var cp = uniform(0, 1)

  // Background probability of E
  var b = uniform(0, 1)

  mapData({data: observedData}, function(datum) {
    // The noisy causal relation to get E given C
    var E = (datum.C &amp;&amp; flip(cp)) || flip(b)
    condition(E == datum.E)
  })

  return {causal_power: cp, background: b}
});

viz.marginals(causalPowerPost);
</code></pre></div></div>

<h4 id="a-2">a)</h4>

<p>Find a set of observations that result in inferring a fairly high causal power for C and a low background probability of E. Explain why this works.</p>

<h4 id="b-2">b)</h4>

<p>Find a set of observations that result in inferring a fairly low causal power for C and a high background probability of E. Explain why this works.</p>

<h4 id="c">c)</h4>

<p>Find a set of observations that result in inferring a fairly high causal power for C and a high background probability of E. Explain why this works.</p>

<h4 id="d">d)</h4>

<p>Suppose every time C is present, so is the effect E. Suppose C is present at least 5 times. Is there a way to nonetheless fail to infer a high causal power for C?</p>



<!--
<a href="Further Reading">/readings/learning-as-conditional-inference.md</a>
<a href="Exercises">/exercises/learning-as-conditional-inference.md</a>
-->



	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	


<!-- put big scripts at end so we don't block page load -->
<script src="../assets/js/webppl.min.js"></script>
<script src="../assets/js/webppl-editor.min.js"></script>
<script src="../assets/js/webppl-viz.min.js"></script>


    </div>

  </body>

<!-- Mirrored from probmods.org/exercises/learning-as-conditional-inference.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 04 Feb 2020 14:10:23 GMT -->
</html>
