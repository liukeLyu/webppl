<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from probmods.org/exercises/inference-algorithms.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 04 Feb 2020 14:10:23 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Algorithms for Inference - exercises</title>

    <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="../assets/css/bootstrap-theme.min.css">
    <link rel="stylesheet" href="../assets/css/default.css">
    <link href="https://fonts.googleapis.com/css?family=Crimson+Text|Inconsolata" rel="stylesheet">
    <script src="../assets/js/ga.js"></script>
    <script src="../assets/js/jquery.min.js"></script>
    <script type="text/javascript" src="../assets/js/bootstrap.min.js"></script>

<!--    <script src="/assets/js/underscore-min.js"></script> 
    <script src="https://probmods.org/bower_components/underscore/underscore.js"></script> -->

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    
      
      <link rel="stylesheet" href="../assets/css/katex.min.css" media="screen" type="text/css">
      
      <link rel="stylesheet" href="../assets/css/littlefoot.css" media="screen" type="text/css">
      
      <link rel="stylesheet" href="../assets/css/webppl-viz.css" media="screen" type="text/css">
      
      <link rel="stylesheet" href="../assets/css/webppl-editor.css" media="screen" type="text/css">
      
    
    
    
    
      
      <script src='../assets/js/katex.min.js' type="text/javascript"></script>
      
      <script src='../assets/js/littlefoot.min.js' type="text/javascript"></script>
      
      <script src='../assets/js/paper-full.js' type="text/javascript"></script>
      
      <script src='../assets/js/parse-bibtex.js' type="text/javascript"></script>
      
      <script src='../assets/js/chapter.js' type="text/javascript"></script>
      
    
    
    
    
  </head>
  <body>

    

    <div class="container">
      <ul class="nav navbar-nav">
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">&#9776;</a>
          <ul class="dropdown-menu">
            <li><a href="../index.html">Home</a></li>
            <li role="separator" class="divider"></li>
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            <li><a href="../chapters/introduction.html">Introduction</a></li>
            
            
            
            <li><a href="../chapters/generative-models.html">Generative models</a></li>
            
            
            
            <li><a href="../chapters/conditioning.html">Conditioning</a></li>
            
            
            
            <li><a href="../chapters/dependence.html">Causal and statistical dependence</a></li>
            
            
            
            <li><a href="../chapters/conditional-dependence.html">Conditional dependence</a></li>
            
            
            
            <li><a href="../chapters/bayesian-data-analysis.html">Bayesian data analysis</a></li>
            
            
            
            <li><a href="../chapters/inference-algorithms.html">Algorithms for inference</a></li>
            
            
            
            <li><a href="../chapters/process-models.html">Rational process models</a></li>
            
            
            
            <li><a href="../chapters/learning-as-conditional-inference.html">Learning as conditional inference</a></li>
            
            
            
            <li><a href="../chapters/lot-learning.html">Learning with a language of thought</a></li>
            
            
            
            <li><a href="../chapters/hierarchical-models.html">Hierarchical models</a></li>
            
            
            
            <li><a href="../chapters/occams-razor.html">Occam's Razor</a></li>
            
            
            
            <li><a href="../chapters/function-learning.html">Learning (deep) continuous functions</a></li>
            
            
            
            <li><a href="../chapters/mixture-models.html">Mixture models</a></li>
            
            
            
            <li><a href="../chapters/social-cognition.html">Social cognition</a></li>
            
            
            
            <li><a href="../chapters/appendix-js-basics.html">Appendix - JavaScript basics</a></li>
            
            
            
            <li><a href="../chapters/appendix-useful-distributions.html">Appendix - Useful distributions</a></li>
            
            
          </ul>
        </li>
      </ul>

      <div class="page-header">
  <h1>Algorithms for Inference - exercises</h1>
</div>

<h2 id="exercise-1-sampling-implicit-curves">Exercise 1. Sampling Implicit Curves</h2>

<p>In the code box below, the <code class="highlighter-rouge">curve</code> function defines a vaguely heart-shaped curve. Below, we use rejection sampling to sample points along the boundary of the curve.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// takes z = 0 cross section of heart surface to some tolerance
// see http://mathworld.wolfram.com/HeartSurface.html
var onCurve = function(x, y) {
  var x2 = x*x;
  var term1 = y - Math.pow(x2, 1/3);
  var crossSection = x2 + term1*term1 - 1;
  return Math.abs(crossSection) &lt; 0.01;
};
var xbounds = [-1, 1];
var ybounds = [-1, 1.6];

var xmu = 0.5 * (xbounds[0] + xbounds[1]);
var ymu = 0.5 * (ybounds[0] + ybounds[1]);
var xsigma = 0.5 * (xbounds[1] - xbounds[0]);
var ysigma = 0.5 * (ybounds[1] - ybounds[0]);

var model = function() {
  var x = gaussian(xmu, xsigma);
  var y = gaussian(ymu, ysigma);
  condition(onCurve(x, y));
  return {x: x, y: y};
};

var post = Infer({method: 'rejection', samples: 1000}, model);
viz.auto(post);
</code></pre></div></div>

<h3 id="a">a)</h3>

<p>Try using MCMC with Metropolis-Hastings instead of rejection sampling. You’ll notice that it does not fare as well as rejection sampling. Why not?</p>

<h3 id="b">b)</h3>

<p>Change the <em>model</em> to make MH successfully trace the curves. Your solution should result in a graph that clearly traces a heart-shaped figure – though it need not do quite as well as rejection sampling. Why does this work better?</p>

<p>HINT: is there a way you can sample a single (x,y) pair instead of separately sampling x and then y? You might want to check out the distribution <a href="https://webppl.readthedocs.io/en/master/distributions.html#DiagCovGaussian">DiagCovGaussian()</a> in the docs. Note that it expects parameters to be <a href="https://webppl.readthedocs.io/en/master/functions/tensors.html#Vector">Vectors</a> and you can extract elements from vectors with <code class="highlighter-rouge">T.get</code> (<code class="highlighter-rouge">T</code> is webppl shorthand for <code class="highlighter-rouge">ad.tensor</code>: for more information on tensor functions, see <a href="https://github.com/dritchie/adnn/blob/master/ad/README.md#available-ad-primitive-functions">adnn docs</a>).</p>

<h3 id="c">c)</h3>

<p>Now change the the inference <em>algorithm</em> (with the original model) to successfully trace the curves. What parameters worked best? <em>Why</em> does this inference algorithm work better than MH?</p>

<p>HINT: you may want to explore HMC! start with the default parameters specified in the HMC <a href="https://webppl.readthedocs.io/en/master/inference/methods.html#mcmc">docs</a> and play with different values.</p>

<h2 id="exercise-2-properties-and-pitfalls-of-metropolis-hastings">Exercise 2. Properties and pitfalls of Metropolis-Hastings</h2>

<p>Consider a very simple function that interpolates between two endpoints.</p>

<p>Suppose one endpoint is fixed at <code class="highlighter-rouge">-10</code>, but we have uncertainty over the value of the other endpoint and the interpolation weight between them. By conditioning on the resulting value being close to 0, we can infer what the free variables must have been:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var interpolate = function(point1, point2, interpolationWeight) {
  return (point1 * interpolationWeight +
          point2 * (1 - interpolationWeight))
}

var model = function(){
  var point1 = -10;
  var point2 = uniform(-100,100);
  var interpolationWeight = uniform(0,1);
  var pointInMiddle = interpolate(point1, point2, interpolationWeight);
  observe(Gaussian({mu: 0, sigma:0.1}), pointInMiddle)
  return {point2, interpolationWeight, pointInMiddle}
}

var posterior = Infer({
  method: 'MCMC',
  samples: 5000,
  lag: 100,
}, model)

var samples = posterior.samples;
viz(marginalize(posterior, function(x) {return x.pointInMiddle}))

// Store these for future use
editor.put("posterior", posterior)
editor.put("samples", samples)
</code></pre></div></div>

<p>By looking at the marginal distribution of <code class="highlighter-rouge">pointInMiddle</code>, we can see that <code class="highlighter-rouge">Infer()</code> successfully finds values of <code class="highlighter-rouge">point2</code> and <code class="highlighter-rouge">interpolationWeight</code> that satisfy our condition.</p>

<h3 id="a-1">a)</h3>

<p>Visualize the separate marginal distributions of <code class="highlighter-rouge">point2</code> and <code class="highlighter-rouge">interpolationWeight</code>. How would you describe their shapes, compared to the marginal distribution of <code class="highlighter-rouge">pointInMiddle</code>?</p>

<p>Now visualize the <em>joint</em> marginal distribution of point2 and interpolationWeight. What does this tell you about their dependence?</p>

<p>HINT: use the <a href="http://docs.webppl.org/en/master/functions/other.html#marginalize">marginalize</a> helper to elegantly construct these marginal distributions</p>

<h3 id="b-1">b)</h3>

<p>WebPPL also exposes the list of MCMC samples that the density plots above are built from. This is saved in the <code class="highlighter-rouge">samples</code> variable. Decrease the number of samples to <code class="highlighter-rouge">50</code> (and the <code class="highlighter-rouge">lag</code> to 0) and plot <code class="highlighter-rouge">pointInMiddle</code> as a function of the sample number. Run this several times to get a feel for the shape of this curve. What do you notice? What property of MCMC are you observing?</p>

<p>HINT: this will require some ‘data munging’ on the array of samples. Some useful functions will be <a href="http://docs.webppl.org/en/master/functions/arrays.html#map"><code class="highlighter-rouge">map</code></a>, <code class="highlighter-rouge">_.range()</code>, and <code class="highlighter-rouge">viz.line</code> which takes arrays <code class="highlighter-rouge">x</code> and <code class="highlighter-rouge">y</code>.</p>

<h3 id="c-1">c)</h3>

<p>Try re-writing the model to use rejection sampling. Note that you will need to find a way to turn the <code class="highlighter-rouge">observe</code> statement into a <code class="highlighter-rouge">condition</code> statement (Hint: See Exercise #1). Is using rejection sampling here a good idea? Why or why not?</p>

<h3 id="d">d)</h3>

<p>Add <code class="highlighter-rouge">verbose: true</code> to the list of options and run <code class="highlighter-rouge">MH</code> again. What is the acceptance rate over time (i.e. what proportion of proposals are actually accepted in the chain?). What about the model puts it at this level?</p>

<p>Consider the list of built-in drift kernels <a href="https://webppl.readthedocs.io/en/master/driftkernels.html?highlight=drift%20kernel#helpers">here</a>. Which of these would be appropriate to use in your model in place of the current uniform prior from which <code class="highlighter-rouge">point2</code> is sampled? After using that kernel in your model, what effect do you observe on the acceptence rate, and why?</p>

<!--
### c)

Edit the code below to implement your Metropolis-Hastings recipe. Use `viz.marginals` to show that it reliably chooses values of `y` and `w` that satisfy the condition.

Hint 1: Check out possible [WebPPL distributions](https://webppl.readthedocs.io/en/master/distributions.html) you might use. 

Hint 2: Many WebPPL distributions require vectors as input. Turn an array into a vector with the function `Vector()` (e.g., `Vector([x, y])`).

Hint 3: Remember that `dist.score(x)` returns the log probability (density) of `x` given distribution `dist`. To turn that into a a probability, use `Math.exp()`. 


## Exercise 4. Topic models

[Topic models](https://en.wikipedia.org/wiki/Topic_model) are a popular method for classifying texts. A "topic" is a probability distribution over a vocabulary. Importantly, different topics have different distributions: a topic pertaining to animals will have higher probability on "wolf" than a topic pertaining to programming. Crucially, different documents are assumed to be generated by drawing words from one or more topics. The job of the model is to, based on some set of documents, infer the latent topics, their probability distributions, and which topics are implicated in which documents. 

Topic models are an example of a [mixture model](11-mixture-models.html). The following code box shows a very simple mixture model, in which each data point was generated by one of three Gaussian distributions, each with its own mean and standard deviation. The variable `weights` represents the relative proportion of data generated by each Gaussian. For instance, the first Gaussian generates 40% of the data. We can use MCMC to recover the means, standard deviations, and weights.

~~~~
var mus = [-2, 0, 2];
var sigmas = [0.25, 1, 0.5];
var weights = [0.4, 0.1, 0.5];

var data = repeat(100, function() {
  var i = discrete(weights);
  return gaussian(mus[i], sigmas[i]);
});

var gaussianMixtureModel = function() {
  var weights = dirichlet(Vector([1, 1, 1]));
  var mus = repeat(3, function() { return gaussian(0, 1); });
  var sigmas = repeat(3, function() { return Math.exp(gaussian(0, 1)); });
  map(function(d) {
    var i = discrete(weights);
    factor(Gaussian({mu: mus[i], sigma: sigmas[i]}).score(d));
  }, data);
  return {mus: mus, sigmas: sigmas, weights: weights};
};

var post = Infer({
  method: 'MCMC',
  steps: 1000
}, gaussianMixtureModel);

print(sample(post))
~~~~

Note that the order of the Gaussians returned by `post` won't necessarily be the same as in `mus`. Thus, we may see a result like this:

```
{"mus":[2.0588258375411383,-1.502841653870516,-0.3507690954089829],"sigmas":[0.7751841178726206,0.8098945135050652,1.411033688748035],"weights":{"dims":[3,1],"length":3,"data":{"0":0.12358930290684293,"1":0.35801309847048407,"2":0.5183975986226731}}}
```

The Gaussian with the mean near `2` is listed first rather than last. You may see a different ordering. We return to this issue in (b) and (c), below.

### a) 

In the model below, we are presented with six very boring texts. Implement a topic model that will infer the probability distribution across words for each of two topics. 

~~~~
var vocabulary = ['DNA', 'evolution', 'parsing', 'phonology'];
var eta = ones([vocabulary.length, 1])

var numTopics = 2
var alpha = ones([numTopics, 1])

var corpus = [
  'DNA evolution DNA evolution DNA evolution DNA evolution DNA evolution'.split(' '),
  'DNA evolution DNA evolution DNA evolution DNA evolution DNA evolution'.split(' '),
  'DNA evolution DNA evolution DNA evolution DNA evolution DNA evolution'.split(' '),
  'parsing phonology parsing phonology parsing phonology parsing phonology parsing phonology'.split(' '),
  'parsing phonology parsing phonology parsing phonology parsing phonology parsing phonology'.split(' '),
  'parsing phonology parsing phonology parsing phonology parsing phonology parsing phonology'.split(' ')
]

var model = function() {

  var topics = repeat(numTopics, function() {
    return T.toScalars(dirichlet({alpha: eta}))
  })

  mapData({data: corpus}, function(doc) {
  
  		// your code here

    })
  })

  return topics
}

var results = // your code here

//plot expected probability of each word, for each topic:
print("Topic 1:")
viz.bar(vocabulary, map(function(i) {return expectation(results, function(v) {return v[0][i]})}, _.range(vocabulary.length)))
print("Topic 2:")
viz.bar(vocabulary, map(function(i) {return expectation(results, function(v) {return v[1][i]})}, _.range(vocabulary.length)))
~~~~

### b)

Run your code from (a) several times. You should see that sometimes Topic 1 favors the words 'DNA' and 'evolution' and Topic 2 favors 'parsing' and 'phonology'. Other times, this is reversed, with Topic 1 favoring 'parsing' and 'phonology' and Topic 2 favoring 'DNA' and 'evolution'.

Why is this? 

### c)

If we ran MCMC on the model in (a) for an infinite amount of time, we would no longer see a distinction between Topic 1 and Topic 2. Why?

Given the answer to that question, why does our model in (a) seem to work?

HINT: We do not need to run our initial mixture model example above nearly as long to get the same effect. This is why we printed samples from the distribution. 
-->



<!--
<a href="Further Reading">/readings/inference-algorithms.md</a>
<a href="Exercises">/exercises/inference-algorithms.md</a>
-->



	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	


<!-- put big scripts at end so we don't block page load -->
<script src="../assets/js/webppl.min.js"></script>
<script src="../assets/js/webppl-editor.min.js"></script>
<script src="../assets/js/webppl-viz.min.js"></script>


    </div>

  </body>

<!-- Mirrored from probmods.org/exercises/inference-algorithms.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 04 Feb 2020 14:10:23 GMT -->
</html>
