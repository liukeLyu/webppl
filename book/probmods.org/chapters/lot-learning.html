<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from probmods.org/chapters/lot-learning.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 04 Feb 2020 14:08:47 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Learning with a language of thought</title>

    <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="../assets/css/bootstrap-theme.min.css">
    <link rel="stylesheet" href="../assets/css/default.css">
    <link href="https://fonts.googleapis.com/css?family=Crimson+Text|Inconsolata" rel="stylesheet">
    <script src="../assets/js/ga.js"></script>
    <script src="../assets/js/jquery.min.js"></script>
    <script type="text/javascript" src="../assets/js/bootstrap.min.js"></script>

<!--    <script src="/assets/js/underscore-min.js"></script> 
    <script src="https://probmods.org/bower_components/underscore/underscore.js"></script> -->

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    
      
      <link rel="stylesheet" href="../assets/css/katex.min.css" media="screen" type="text/css">
      
      <link rel="stylesheet" href="../assets/css/littlefoot.css" media="screen" type="text/css">
      
      <link rel="stylesheet" href="../assets/css/webppl-viz.css" media="screen" type="text/css">
      
      <link rel="stylesheet" href="../assets/css/webppl-editor.css" media="screen" type="text/css">
      
    
    
    
    
      
      <script src='../assets/js/katex.min.js' type="text/javascript"></script>
      
      <script src='../assets/js/littlefoot.min.js' type="text/javascript"></script>
      
      <script src='../assets/js/paper-full.js' type="text/javascript"></script>
      
      <script src='../assets/js/parse-bibtex.js' type="text/javascript"></script>
      
      <script src='../assets/js/chapter.js' type="text/javascript"></script>
      
    
    
    
    
  </head>
  <body>

    

    <div class="container">
      <ul class="nav navbar-nav">
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">&#9776;</a>
          <ul class="dropdown-menu">
            <li><a href="../index.html">Home</a></li>
            <li role="separator" class="divider"></li>
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            <li><a href="introduction.html">Introduction</a></li>
            
            
            
            <li><a href="generative-models.html">Generative models</a></li>
            
            
            
            <li><a href="conditioning.html">Conditioning</a></li>
            
            
            
            <li><a href="dependence.html">Causal and statistical dependence</a></li>
            
            
            
            <li><a href="conditional-dependence.html">Conditional dependence</a></li>
            
            
            
            <li><a href="bayesian-data-analysis.html">Bayesian data analysis</a></li>
            
            
            
            <li><a href="inference-algorithms.html">Algorithms for inference</a></li>
            
            
            
            <li><a href="process-models.html">Rational process models</a></li>
            
            
            
            <li><a href="learning-as-conditional-inference.html">Learning as conditional inference</a></li>
            
            
            
            <li><a href="lot-learning.html">Learning with a language of thought</a></li>
            
            
            
            <li><a href="hierarchical-models.html">Hierarchical models</a></li>
            
            
            
            <li><a href="occams-razor.html">Occam's Razor</a></li>
            
            
            
            <li><a href="function-learning.html">Learning (deep) continuous functions</a></li>
            
            
            
            <li><a href="mixture-models.html">Mixture models</a></li>
            
            
            
            <li><a href="social-cognition.html">Social cognition</a></li>
            
            
            
            <li><a href="appendix-js-basics.html">Appendix - JavaScript basics</a></li>
            
            
            
            <li><a href="appendix-useful-distributions.html">Appendix - Useful distributions</a></li>
            
            
          </ul>
        </li>
      </ul>

      <div class="page-header">
  <h1>Learning with a language of thought</h1>
</div>

<p>An important worry about Bayesian models of learning is that the Hypothesis space must either be too simple (e.g. a single coin weight!), specified in a rather ad-hoc way, or both. There is a tension here: human representations of the world are enormously complex and so the space of possible representations must be correspondingly big, and yet we would like to understand the representational resources in simple and uniform terms. How can we construct very large (possibly infinite) hypothesis spaces and priors over them? One possibility is to build the hypotheses themselves via <em>stochastic recursion</em>. That is, we build hypotheses by a combination of primitives and combination operations, randomly deciding which to use.</p>

<p>For instance, imagine that we want a model that generates strings, but we want the strings to be valid arithmetic expressions. Since we know that arithmetic has as primitives numbers and combines them with operations, we can define a simple generator:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var randomConstant = function() {
  return uniformDraw(_.range(10))
}

var randomCombination = function(f,g) {
  var op = uniformDraw(['+','-','*','/','^']);
  return '('+f+op+g+')'
}

// sample an arithmetic expression
var randomArithmeticExpression = function() {
  flip() ? 
    randomCombination(randomArithmeticExpression(), randomArithmeticExpression()) : 
    randomConstant()
}

randomArithmeticExpression()
</code></pre></div></div>

<p>Notice that <code class="highlighter-rouge">randomArithmeticExpression</code> can generate an infinite set of different strings, but that more complex strings are less likely. That is, the process we use to build strings also (implicitly) defines a prior over strings that penalizes complexity. To see this more  let’s use Infer to reveal the first 100 strings:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var randomConstant = function() {
  return uniformDraw(_.range(10))
}

var randomCombination = function(f,g) {
  var op = uniformDraw(['+','-','*','/','^']);
  return '('+f+op+g+')'
}

// sample an arithmetic expression
var randomArithmeticExpression = function() {
  flip() ? 
    randomCombination(randomArithmeticExpression(), randomArithmeticExpression()) : 
    randomConstant()
}

randomArithmeticExpression()
viz.table(Infer({method: 'enumerate', maxExecutions: 100}, function() {
  return randomArithmeticExpression()
}))
</code></pre></div></div>

<p>If we now interpret our strings as <em>hypotheses</em>, we have compactly defined an infinite hypothesis space and its prior.</p>

<h2 id="example-inferring-an-arithmetic-expression">Example: Inferring an Arithmetic Expression</h2>

<p>Consider the following WebPPL program, which induces an arithmetic function from examples. It is much as above, except that we include a variable among the primitives (to make it a function). We also construct a nested array, instead of a simple string, and include a function <code class="highlighter-rouge">runify</code> that converts the array into a javascript function.
(The helper function <code class="highlighter-rouge">prettify</code>, above the fold, makes the nested array pretty to look at.)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>///fold:
// make expressions easier to look at
var prettify = function(e) {
  if (e == 'x' || _.isNumber(e)) {
    return e
  } else {
    var op = e[0]
    var arg1 = prettify(e[1])
    var prettyarg1 = (!_.isArray(e[1]) ? arg1 : '(' + arg1 + ')')
    var arg2 = prettify(e[2])
    var prettyarg2 = (!_.isArray(e[2]) ? arg2 : '(' + arg2 + ')')
    return prettyarg1 + ' ' + op + ' ' + prettyarg2
  }
}
///

// make expressions runnable
var runify = function(e) {
  //helper functions:
  var plus = function(a,b) {return a + b}
  var multiply = function(a,b) {return Math.round(a * b,0)}
  var divide = function(a,b) {return Math.round(a/b,0)}
  var minus = function(a,b) {return a - b}
  var power = function(a,b) {return Math.pow(a,b)}
  var identity = function(a) {return a}
  
  if (e == 'x') {
    return identity
  } else if (_.isNumber(e)) {
    return function(z) { return e }
  } else {
    var op = (e[0] == '+') ? plus : 
             (e[0] == '-') ? minus :
             (e[0] == '*') ? multiply :
             (e[0] == '/') ? divide :
              power
    var arg1Fn = runify(e[1])
    var arg2Fn = runify(e[2])
    return function(z) {
      return op(arg1Fn(z),arg2Fn(z))
    }
  }
}

var randomConstantFunction = function() {
  return uniformDraw(_.range(10))
}

var randomCombination = function(f,g) {
  var op = uniformDraw(['+','-','*','/','^']);
  return [op, f, g];
}

// sample an arithmetic expression
var randomArithmeticExpression = function() {
  if (flip()) {
    return randomCombination(randomArithmeticExpression(), randomArithmeticExpression())
  } else {
    if (flip()) {
      return 'x'
    } else {
      return randomConstantFunction()
    }
  }
}

viz.table(Infer({method: 'enumerate', maxExecutions: 1000}, function() {
  var e = randomArithmeticExpression();
  var s = prettify(e);
  var f = runify(e);
  condition(f(1) == 3);

  return {s: s};
}))
</code></pre></div></div>

<p>This model can learn any function consisting of the integers 0 to 9 and the operations add, subtract, multiply, divide, and raise to a power. 
The condition in this case asks for an arithmetic expression on variable <code class="highlighter-rouge">x</code> such that it evaluates to <code class="highlighter-rouge">3</code> when <code class="highlighter-rouge">x</code> is <code class="highlighter-rouge">1</code>. There are many extensionally equivalent ways to satisfy the condition, for instance the expressions <code class="highlighter-rouge">3</code>, <code class="highlighter-rouge">1 + 2</code>, and <code class="highlighter-rouge">x + 2</code>, but because the more complex expressions require more choices to generate, they are chosen less often.</p>

<p>Notice that the model puts the most probability on a function that always returns <code class="highlighter-rouge">3</code> (<script type="math/tex">f(x) = 3</script>). This is the simplest hypothesis consistent with the data. Let’s see what happens if we have more data – try changing the condition in the above query to <code class="highlighter-rouge">condition(f(1) == 3 &amp;&amp; f(2) == 4)</code>, then to <code class="highlighter-rouge">condition(f(1) == 3 &amp;&amp; f(2) == 6)</code>.</p>

<p>This model learns from an infinite hypothesis space—all expressions made from ‘x’, ‘+’, ‘-‘, and constant integers—but specifies both the hypothesis space and its prior using the simple generative process <code class="highlighter-rouge">randomArithmeticExpression</code>.</p>

<h2 id="example-rational-rules">Example: Rational Rules</h2>

<p>How can we account for the productivity of human concepts (the fact that every child learns a remarkable number of different, complex concepts)? The “classical” theory of concepts formation accounted for this productivity by hypothesizing that concepts are represented compositionally, by logical combination of the features of objects (see for example Bruner, Goodnow, and Austin, 1951). That is, concepts could be thought of as rules for classifying objects (in or out of the concept) and concept learning was a process of deducing the correct rule.</p>

<p>While this theory was appealing for many reasons, it failed to account for a variety of categorization experiments. Here are the training examples, and one transfer example, from the classic experiment of Medin and Schaffer (1978). The bar graph above the stimuli shows the portion of human participants who said that bug was a “fep” in the test phase (the data comes from a replication by Nosofsky, Gluck, Palmeri, McKinley (1994); the bug stimuli are courtesy of Pat Shafto):</p>

<p><img src="../assets/img/Medin54-bugs.png" width="500" /></p>

<p>Notice three effects: there is a gradient of generalization (rather than all-or-nothing classification), some of the Feps are better (or more typical) than others (this is called “typicality”), and the transfer item is a ‘‘better’’ Fep than any of the Fep exemplars (this is called “prototype enhancement”). Effects like these were difficult to capture with classical rule-based models of category learning, which led to deterministic behavior. As a result of such difficulties, psychological models of category learning turned to more uncertain, prototype and exemplar based theories of concept representation. These models were able to predict behavioral data very well, but lacked  compositional conceptual structure.</p>

<p>Is it possible to get graded effects from rule-based concepts? Perhaps these effects are driven by uncertainty in <em>learning</em> rather than uncertainty in the representations themselves? To explore these questions Goodman, Tenenbaum, Feldman, and Griffiths (2008) introduced the Rational Rules model, which learns deterministic rules by probabilistic inference. This model has an infinite hypothesis space of rules (represented in propositional logic), which are generated compositionally. Here is a slightly simplified version of the model, applied to the above experiment:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// first set up the training and test data:
var numFeatures = 4;

var makeObj = function(l) {return _.zipObject(['trait1', 'trait2', 'trait3', 'trait4', 'fep'], l)}
var feps = map(makeObj, [[0,0,0,1, 1], [0,1,0,1, 1], [0,1,0,0, 1], [0,0,1,0, 1], [1,0,0,0, 1]])
var nonFeps = map(makeObj, [[0,0,1,1, 0], [1,0,0,1, 0], [1,1,1,0, 0], [1,1,1,1, 0]])
var others = map(makeObj, [[0,1,1,0], [0,1,1,1], [0,0,0,0], [1,1,0,1], [1,0,1,0], [1,1,0,0], [1,0,1,1]])
var data = feps.concat(nonFeps)
var allObjs = others.concat(feps).concat(nonFeps)

//here are the human results from Nosofsky et al, for comparison:
var humanFeps = [.77, .78, .83, .64, .61]
var humanNonFeps = [.39, .41, .21, .15]
var humanOther = [.56, .41, .82, .40, .32, .53, .20]
var humanData = humanOther.concat(humanFeps).concat(humanNonFeps)

// two parameters: stopping probability of the grammar, and noise probability:
var tau = 0.3;
var noiseParam = Math.exp(-1.5)

// a generative process for disjunctive normal form propositional equations:
var samplePred = function() {
  var trait = uniformDraw(['trait1', 'trait2', 'trait3', 'trait4'])
  var value = flip()
  return function(x) {return x[trait] == value}
}

var sampleConj = function() {
  if(flip(tau)) {
    var c = sampleConj()
    var p = samplePred()
    return function(x) {return c(x) &amp;&amp; p(x)}
  } else {
    return samplePred()
  }
}

var getFormula = function() {
  if(flip(tau)) {
    var c = sampleConj()
    var f = getFormula()
    return function(x) {return c(x) || f(x)};
  } else {
    return sampleConj()
  }
}

var rulePosterior = Infer({method: 'enumerate', maxExecutions: 1000}, function() {
  // sample a classification formula
  var rule = getFormula()
  
  // condition on correctly (up to noise) accounting for observations
  var obsFn = function(datum){observe(Bernoulli({p: rule(datum) ? (1-noiseParam) : noiseParam}), datum.fep == 1)}
  mapData({data: data}, obsFn) 
  
  // return posterior predictive
  return map(rule, allObjs)
})

//build predictive distribution for each item
var predictives = map(function(i){expectation(rulePosterior,function(x){x[i]})}, _.range(15))

viz.scatter(predictives, humanData)
</code></pre></div></div>

<!-- TODO: try fullboolean language. switch to that in main text or show both? or maybe as an exercise? -->

<p>In addition to achieving a good overall correlation with the data, this model captures the three qualitative effects described above: graded generalization, typicality, and prototype enhancement. Make sure you see how to read each of these effects from the above plot!
Goodman, et al, have used to this model to capture a variety of other classic categorization effects [@Goodman2008b], as well. Thus probabilistic induction of (deterministic) rules can capture many of the graded effects previously taken as evidence against rule-based models.</p>

<h1 id="grammar-based-induction">Grammar-based induction</h1>

<p>What is the general principle in the two above examples? We can think of it as the following recipe: we build hypotheses by stochastically choosing between primitives and combination operations, this specifies an infinite “language of thought”; each expression in this language in turn specifies the likelihood of observations.
Formally, the stochastic combination process specifies a probabilistic grammar; which yields terms compositionally interpreted into a likelihood over data. 
A small grammar can generate an infinite array of potential hypotheses; because grammars are themselves generative processes, a prior is provided for free from this formulation.</p>

<!--
TODO: 
what's a grammar?
compositionality and frege's principle
-->

<p>This style of compositional concept induction model, can be naturally extended to complex hypothesis spaces, each defined by a grammar. For instance to model theory acquisition, learning natural numbers concepts, and many others. See:</p>

<ul>
  <li>
    <p>Compositionality in rational analysis: Grammar-based induction for concept learning. N. D. Goodman, J. B. Tenenbaum, T. L. Griffiths, and J. Feldman (2008). In M. Oaksford and N. Chater (Eds.). The probabilistic mind: Prospects for Bayesian cognitive science.</p>
  </li>
  <li>
    <p>A Bayesian Model of the Acquisition of Compositional Semantics. S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B. Tenenbaum (2008). Proceedings of the Thirtieth Annual Conference of the Cognitive Science Society.</p>
  </li>
  <li>
    <p>Piantadosi, S. T., &amp; Jacobs, R. A. (2016). Four Problems Solved by the Probabilistic Language of Thought. Current Directions in Psychological Science, 25(1).</p>
  </li>
</ul>

<p>There is also no reason that the concepts need to be deterministic; in WebPPL stochastic functions can be constructed compositionally and learned by induction:</p>

<ul>
  <li>Learning Structured Generative Concepts. A. Stuhlmueller, J. B. Tenenbaum, and N. D. Goodman (2010). Proceedings of the Thirty-Second Annual Conference of the Cognitive Science Society.</li>
</ul>

<p>Reading &amp; Discussion: <a href="../readings/lot-learning.html">Readings</a></p>

<p>Test your knowledge: <a href="../exercises/lot-learning.html">Exercises</a></p>


<!--
<a href="Further Reading">/readings/lot-learning.md</a>
<a href="Exercises">/exercises/lot-learning.md</a>
-->



	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
	

	
		Next chapter: <a href="hierarchical-models.html">11. Hierarchical models</a>
		

<!-- put big scripts at end so we don't block page load -->
<script src="../assets/js/webppl.min.js"></script>
<script src="../assets/js/webppl-editor.min.js"></script>
<script src="../assets/js/webppl-viz.min.js"></script>


    </div>

  </body>

<!-- Mirrored from probmods.org/chapters/lot-learning.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 04 Feb 2020 14:08:50 GMT -->
</html>
